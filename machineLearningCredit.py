# -*- coding: utf-8 -*-
"""Untitled3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1GXy_wZVMNECn4GeCTaXx7kI3QJsJxy4D
"""

#As execuções foram feitas linha por linha, dentro do ambiente do google colab

#Importando as bibliotecas
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import plotly.express as px

#Importando Base
base = pd.read_csv('credit_data.csv')

grafico = px.scatter_matrix(base, dimensions=['income', 'age', 'loan'], color='default')
grafico.show()

#Correções de problemas para a base

base.loc[base['age'] < 0, 'age'] = base['age'][base['age']>0].mean()
base['age'].fillna(base['age'].mean(), inplace = True)

#Remove as idades negativas e nulas

#Mostra os dados após a correção deles
grafico = px.scatter_matrix(base, dimensions=['income', 'age', 'loan'], color='default')
grafico.show()

#Agora iremos dividir a base em duas variáveis.
#Uma para os atributos previsores, que serão utilizados para previsão em X.
#E outra para armazenar a classe, em Y.

#Criando a variável para receber previsores.
#A função iloc vai buscar os registros em linhas e colunas [linha inicial: linha final, coluna inicial: coluna final].
#O: sozinho irá indicar que iremos pegar todos os conjuntos de linhas e colunas

#É necessário atenção para não enviar atributos que não sejam interessantes para nós.
#O ID, por exemplo, não vai dar diferença nenhuma para nós.

#Nas consultas, quando indicamos os limites, upper e lower bound, no limite máximo, a consulta não irá pegar ele.
#Ele vai pegar os atributos de 1 até o 3
#Values serve para numpy, que irá converter para o que é o formato utilizado nas bibliotecas que iremos trabalhar

#A classe é o crédito, ou seja, o que queremos  prever, o crédito da pessoa. Atributos previsores, são o que utilizamos para
#Fazer a previsão

x_credit = base.iloc[:, 1:4].values

#Agora, iremos pegar apenas a coluna 4 para preencher o campo de classes.
#Ele irá pegar apenas o 4, pois nenhum intervalo foi especificado.

y_credit = base.iloc[:, 4].values

y_credit

#Filtramos apenas a coluna e verificamos o valor mínimo, e depois o valor máximo, para ver os ranges
#Ela será comparada pegando a idade mínima, sendo comparada com a renda mínima

x_credit[:, 0].min(), x_credit[:, 1].min(), x_credit[:, 2].min()

#Agora, pegaremos a renda máxima, e iremos comparar com a idade máxima

x_credit[:, 0].max(), x_credit[:, 1].max(), x_credit[:, 2].max()

#Agora, iremos importar a sklearn, uma biblioteca para aprendizagem de máquina no python
from sklearn.preprocessing import StandardScaler
scaler_credit = StandardScaler()
x_credit = scaler_credit.fit_transform(x_credit)

#Agora, se rodarmos o código de mínimo novamente, será possível notar que
#Os valores mínimos e máximos estão na mesma escala, ou seja, todos tem média 0 e desvio padrão 1
#Colocando dessa forma, o desempenho do aprendizado de máquina melhora.

x_credit[:, 0].min(), x_credit[:, 1].min(), x_credit[:, 2].min()

x_credit[:, 0].max(), x_credit[:, 1].max(), x_credit[:, 2].max()

#Agora vamos importar uma biblioteca para treinamento e aprendizado de máquina, para fazer os testes
from sklearn.model_selection import train_test_split

#Para organizar nossa base, criaremos 4 variáveis, 2 com atributos previsores (treinamento e teste)
#E duas com classes (treinamento e teste)
# X são os atributos previroes
# E Y são classes

#A função vai gerar divisões de treinamento e teste para X e Y
#Passaremos os atributos previsores, as classes, e o tamanho da base de dados (Test_size)
#Como temos poucos registros, vamos escolher 0.25, se base fosse maior, poderíamos utilizar um valor menor
#random_state = podemos usar qualquer valor, mas precisaremos o definir
#Assim, toda vez que executarmos o código teremos a mesma divisão dos registros, com o random state, teremos resultados iguais
#Pois usamos a mesma base

x_credit_treinamento, x_credit_teste, y_credit_treinamento, y_credit_teste = train_test_split(x_credit, y_credit, test_size = 0.25, random_state = 0)

#O comando abaixo vai mostrar o número de registros, e o número de colunas
x_credit_treinamento.shape

#No caso abaixo, também temos as mesmas 1500 linhas, mas apenas uma coluna default
y_credit_treinamento.shape

x_credit_teste.shape, y_credit_teste.shape

#Agora, salvaremos a base utilizando a biblitoeca pickle do python
#Ela vai salvar as variáveis do disco, utilizando a extensão pkl

import pickle

#Com o código abaixo, definiremos o nome do arquivo com a extensão
with open('credit.pkl', mode = 'wb') as f:
  pickle.dump([x_credit_treinamento, y_credit_treinamento, x_credit_teste, y_credit_teste], f)
#São passadas as variáveis que vão ser salvas em formato de lista
#Depois, ele vai gerar um arquivo com o nome contendo as variáveis.

#Importação da análise gaugasiana
from sklearn.naive_bayes import GaussianNB

#Agora, carregaremos o algoritmo que vai ser utilizado para a análise
naive_credit_data = GaussianNB()

#Aqui geramos a tabela de probabilidades
naive_credit_data.fit(x_credit_treinamento, y_credit_treinamento)

#Aqui será verificado o teste
previsoes = naive_credit_data.predict(x_credit_teste)

previsoes

y_credit_teste

from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
#Carregamento da biblioteca de análise de resultados

accuracy_score(y_credit_teste, previsoes)
#Avaliação das previsões, em comparação às respostas corretas

#Matriz de confusão
confusion_matrix(y_credit_teste, previsoes)

#428 0, ou seja, pagam, 8 pagantes que o algoritmo respondeu como 1. 23 Não pagam que foram como 0
#E 41 não pagam, que foram como não pagantes

#Importação de uma biblioteca para visualização
from yellowbrick.classifier import ConfusionMatrix

cm = ConfusionMatrix(naive_credit_data)
cm.fit(x_credit_treinamento, y_credit_treinamento)
cm.score(x_credit_teste, y_credit_teste)

#98% dos clientes que pagam, com 95% de precisão
#64% dos clientes que não pagam, com 84% de precisão

print(classification_report(y_credit_teste, previsoes))

#Importando as bibliotecas necessárias para o treinamento
from sklearn.tree import DecisionTreeClassifier
from sklearn import tree
import matplotlib.pyplot as plt

#Carregando as informações para treinar o modelo
with open('credit.pkl', 'rb') as f:
  x_credit_treinamento, y_credit_treinamento, x_credit_teste, y_credit_teste = pickle.load(f)

x_credit_treinamento.shape, y_credit_treinamento.shape

#Definindo os parâmetros para a aprendizagem
arvore_risco = DecisionTreeClassifier(criterion='entropy')

#Executando o treinamento conforme os parâmetros definidos
arvore_risco.fit(x_credit_treinamento, y_credit_treinamento)

#Apresentando o grau de importância de cada uma das características
arvore_risco.feature_importances_

#Montando o gráfico com a área de decisão do algoritmo
#Montando os cabeçalhos
previsores = ['Salário', 'Idade', 'Divida']

#Estruturando o gráfico
figura, eixos = plt.subplots(nrows = 1, ncols = 1, figsize = (15,15))

#Plotando a árvore
tree.plot_tree(arvore_risco, feature_names=previsores, filled=True)

#Salvando a figura
figura.savefig('arvore_risco.png')

#Verificando o aprendizado de árvore com os dados apresentados
previsoes_arvore = arvore_risco.predict(x_credit_teste)

previsoes_arvore

#Verificando as respostas
y_credit_teste

#Avaliando a acurácia
acuracia_arvore = accuracy_score(y_credit_teste, previsoes_arvore)
print(acuracia_arvore * 100, "%")

#Versão simplificada
confusion_matrix(y_credit_teste, previsoes_arvore)

#Versão elaborada
cm = ConfusionMatrix(arvore_risco)
cm.fit(x_credit_treinamento, y_credit_treinamento)
cm.score(x_credit_teste, y_credit_teste)

#Imprimindo a árvore com os resultados obtidos
print(classification_report(y_credit_teste, previsoes_arvore))